---
layout: post
title: DL(1)
category: deeplearning
---


## DeepLearning与神经网络模型 ##
神经网络模型用来模拟人脑处理信息的过程，关键是抽象和迭代。从原始信号，做低级抽象，逐渐向高级抽象迭代，高层的特征是低层特征的组合，从低层到高层的特征表示越来越抽象，越来越能表现语义。总体来说，神经网络模型和人脑处理信息的过程是分级的，对于自然语言，单词集合和句子的对应是多对一的，句子和语义的对应也是多对一的，语义和意图的对应还是多对一的。

## 结构性特征 ##
颗粒度太小的特征对于DL来说意义不大，如何从数据中提取结构性的特征对于模型是有用的。直观上说，就是找到make sense的最小的patch再将其进行组合，得到上一层的feature，递归向上学习特征。


## DL的基本思想 ##
对于一个n层的分层系统S，信息从输入到输出被逐层处理，这个过程中信息会逐层丢失(信息处理不等式)。
大部分的机器学习算法都可以看做为只有一层隐藏层(或者没有隐藏层)的浅层模型，比如LDA、SVM等；而多隐藏层的人工神经网络具有优异的特征学习能力，训练上的难度可以通过_"逐层初始化"_来有效克服。

如果网络中的每个节点都只使用线性激活函数，那么整个网络的输出也是线性激活函数，线性函数的组合还是线性函数。因此，大多数神经网络都使用非线性激活函数：对数函数、双曲正切函数、阶跃函数、整流函数等。

### Back Propagation反向传播算法 ###
传统的神经网络中，采用的是BP反向传播算法，采用迭代的方法来训练整个网络，随机设定初值，计算当前网络的输出，然后根据当前输出和label之间的差距去调整前面各层的参数，直到收敛。

![BP](http://{{ site.host }}/assets/images/DL1_bp.jpg)

对于深度学习模型，采用BP算法时，残差传播时会出现梯度扩散，达到最前面的层时已经变得太小。

深度结构（涉及多个非线性处理单元层）非凸目标代价函数中普遍存在的局部最小是训练困难的主要来源。

![局部最优](http://{{ site.host }}/assets/images/DL2_minimum.png)

###  隐藏层 ###

一个具有有限数目神经元的隐含层可以被训练成可逼近任意随机函数。

### layer-wise机制 ###
layer-wise机制是在非监督数据上建立多层神经网络的一个有效方法。每次训练一层网络，使原始表示层x向上生成的高级表示层r和该高级表示r向下生成的x'尽可能一致。


	1.首先逐层构建单层神经元，这样每次都是训练一个单层网络，自下而上非监督学习。
	2.当所有层训练完后，使用wake-sleep算法进行调优。

	wake-sleep算法
	将除最顶层的其它层间的权重变为双向的，这样最顶层仍然是一个单层神经网络，而其它层则变为了图模型。向上的权重用于“认知”，向下的权重用于“生成”。然后使用Wake-Sleep算法调整所有的权重。让认知和生成达成一致，也就是保证生成的最顶层表示能够尽可能正确的复原底层的结点。
	
	wake阶段：认知过程，通过外界的特征和向上的权重（认知权重）产生每一层的抽象表示（结点状态），并且使用梯度下降修改层间的下行权重（生成权重）。也就是“如果现实跟我想象的不一样，改变我的权重使得我想象的东西就是这样的”。
	
	sleep阶段：生成过程，通过顶层表示（醒时学得的概念）和向下权重，生成底层的状态，同时修改层间向上的权重。



非监督训练得到各层参数后进一步对调整各个层的参数，自顶向下的监督学习，就是通过带标签的数据去训练，误差自顶向下传输，对网络进行微调。

### AutoEncoder自动编码器 ###

自编码器是一个典型的前馈神经网络，目标是学习一种对数据压缩且分布式的表示方式。

![autoencoder](http://{{ site.host }}/assets/images/DL1_autoEncoder.jpg)

将input输入一个encoder编码器，就会得到一个code，这个code也就是输入的一个表示，那么我们怎么知道这个code表示的就是input呢？我们加一个decoder解码器，这时候decoder就会输出一个信息，那么如果输出的这个信息和一开始的输入信号input是很像的（理想情况下就是一样的），那很明显，我们就有理由相信这个code是靠谱的。所以，我们就通过调整encoder和decoder的参数，使得重构误差最小，这时候我们就得到了输入input信号的第一个表示了，也就是编码code了。

#### 通过编码器产生特征，然后训练下一层。这样逐层训练 ####

将第一层输出的code当成第二层的输入信号，同样最小化重构误差，就会得到第二层的参数，并且得到第二层输入的code，也就是原输入信息的第二个表达了。

#### 分类,有监督微调 ####

为了实现分类，我们就可以在AutoEncoder的最顶的编码层添加一个分类器（例如罗杰斯特回归、SVM等），然后通过标准的多层神经网络的监督训练方法（梯度下降法）去训练(

将最后层的特征code输入到最后的分类器，通过有标签样本，通过监督学习进行微调)。

只调整分类器:

![classify](http://{{ site.host }}/assets/images/DL1_classify.jpg)

通过有标签样本，微调整个系统:

![classify2](http://{{ site.host }}/assets/images/DL1_classify2.jpg)

#### Sparse AutoEncoder 稀疏自动编码器 ####




### RBM 受限玻尔兹曼机 ###



{% include references.md %}
