---
layout: post
title: DL(1)
category: deeplearning
---


###DeepLearning与神经网络模型
神经网络模型用来模拟人脑处理信息的过程，关键是抽象和迭代。从原始信号，做低级抽象，逐渐向高级抽象迭代，高层的特征是低层特征的组合，从低层到高层的特征表示越来越抽象，越来越能表现语义。总体来说，神经网络模型和人脑处理信息的过程是分级的，对于自然语言，单词集合和句子的对应是多对一的，句子和语义的对应也是多对一的，语义和意图的对应还是多对一的。

####结构性特征
颗粒度太小的特征对于DL来说意义不大，如何从数据中提取结构性的特征对于模型是有用的。直观上说，就是找到make sense的最小的patch再将其进行组合，得到上一层的feature，递归向上学习特征。


####DL的基本思想
对于一个n层的分层系统S，信息从输入到输出被逐层处理，这个过程中信息会逐层丢失(信息处理不等式)。
大部分的机器学习算法都可以看做为只有一层隐藏层(或者没有隐藏层)的浅层模型，比如LDA、SVM等；而多隐藏层的人工神经网络具有优异的特征学习能力，训练上的难度可以通过"逐层初始化"来有效克服。

#####Back Propagation反向传播算法
传统的神经网络中，采用的是BP反向传播算法，采用迭代的方法来训练整个网络，随机设定初值，计算当前网络的输出，然后根据当前输出和label之间的差距去调整前面各层的参数，直到收敛。对于深度学习模型，采用BP算法时，残差传播时会出现梯度扩散，达到最前面的层时已经变得太小。
深度结构（涉及多个非线性处理单元层）非凸目标代价函数中普遍存在的局部最小是训练困难的主要来源。

#####layer-wise机制
layer-wise机制是在非监督数据上建立多层神经网络的一个有效方法。每次训练一层网络，使原始表示层x向上生成的高级表示层r和该高级表示r向下生成的x'尽可能一致。

```
1.首先逐层构建单层神经元，这样每次都是训练一个单层网络，自下而上非监督学习。
2.当所有层训练完后，使用wake-sleep算法进行调优。

wake-sleep算法
   将除最顶层的其它层间的权重变为双向的，这样最顶层仍然是一个单层神经网络，而其它层则变为了图模型。向上的权重用于“认知”，向下的权重用于“生成”。然后使用Wake-Sleep算法调整所有的权重。让认知和生成达成一致，也就是保证生成的最顶层表示能够尽可能正确的复原底层的结点。
   
   wake阶段：认知过程，通过外界的特征和向上的权重（认知权重）产生每一层的抽象表示（结点状态），并且使用梯度下降修改层间的下行权重（生成权重）。也就是“如果现实跟我想象的不一样，改变我的权重使得我想象的东西就是这样的”。
   sleep阶段：生成过程，通过顶层表示（醒时学得的概念）和向下权重，生成底层的状态，同时修改层间向上的权重。
```
非监督训练得到各层参数后进一步对调整各个层的参数，自顶向下的监督学习，就是通过带标签的数据去训练，误差自顶向下传输，对网络进行微调。

#####AutoEncoder自动编码器




{% include references.md %}
