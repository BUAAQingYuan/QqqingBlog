---
layout: post
title: Keras框架
category: deeplearning
---


### 核心层 ###

Dense类 

一维全连接层


Dropout

训练和预测时随机减少特征个数(总数*p)，即去掉输入数据中的某些维度，用于防止过拟合。


卷积层

卷积操作的优点就是可以增强原信号特征，增强对原信号位移、形变之后的识别能力，有效降低噪音。

Convolution1D 该卷积操作用于过滤一维输入的相邻元素。

MaxPooling1D  下采样。

Convolution2D 通过一个2维窗口的卷积核进行过滤。

MaxPooling2D  二维下采样

SimpleRNN     一种输出反馈给输入的全连接RNN

### 优化器 ###
编译一个Keras模型，优化器和损失函数是必需的两个参数。


	sgd=SGD(lr=0.1,decay=1e-6,momentum=0.9,nesterov=True)
	model.compile(loss='categorical_crossentropy',optimizer=sgd)

常用的几种优化器:

SGD 随机梯度下降优化器。

	sgd=SGD(lr=0.1,decay=1e-6,momentum=0.9,nesterov=True)
	lr 学习速率
	momentum 参数更新的动量
	decay 每次更新后学习速率的衰减量
	nesterov 是否使用Nesterov动量项

### 损失函数或目标函数 ###
比较常用的是均方误差。损失函数的作用就是返回预测结果与实际值之间的差距，然后优化器更加差值进行参数调整。不同的损失函数之间的区别就是对这个差值的度量方式不同。

常用的损失函数:

mean_squared_error /mse 均方误差

mean_absolute_error/mae 绝对值均差，$$ (\left \| y_{pred}-y_{true} \right \|).mean() $$。

### 模型 ###
Keras可以建立两种模型，一种是线性叠加的，层与层之间是全连接的方式。

	model=keras.models.Sequential()

另外一种是图模型，可以指定层与层之间的部分连接方式。图模型可以有任意数量的输入和输出，每一个输出都由专门的代价函数进行训练。图模型的最优化取决于所有代价函数的总和。

	model=keras.models.Graph()

模型训练的时候首先对权值矩阵和偏置进行初始化。一般使用梯度下降算法的网络使用随机数初始化，随机数初始化一般按照概率分布去取值。

	model.add(Dense(64,init='uniform'))

模型训练时常会出现过拟合的现象，有效解决过拟合的方法之一就是添加规则化项。规则化项是一个对于权值参数的惩罚项，包含在代价函数中。Keras中的layer包含三种规则化项的参数。

W_regularizer  对权值规则化

b_regularizer  对偏置规则化

activity_regularizer  对激活值规则化，权值与矩阵相乘的输出规则化

同时还有当训练网络到最优时对网络参数的约束，这个约束限制参数值的取值范围，比如最大值、不允许为负值、归一化等。

W_constraint      权值约束

b_constraint      约束偏置值




### 激活函数 ###
每一层的网络输出都要经过激活函数。

常用的激活函数

softmax  多分类中常用的激活函数

softplus $$ softplus(x)=log(1+e^{x}) $$，近似于生物神经激活函数

sigmoid  S型曲线激活函数

linear   线性激活函数



{% include references.md %}
