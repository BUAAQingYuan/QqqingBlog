---
layout: post
title: DL(2)
category: deeplearning
---


## 稀疏自编码器 ##
逐层处理信息量不会增加，稀疏编码器中的隐藏层个数比前一层个数要少。隐藏层是一个自编码的过程，在这个过程中尽可能的保留输入的信息。

## Dropout ##
Dropout是训练神经网络模型时，防止模型过拟合的一种训练策略，主要思路是通过阻止特征检测器的共同作用来提高神经网络的性能。
在


+ 1.每次用输入网络的样本进行权值更新时，隐含节点以一点概率随机出现，这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其他特征出现的情况下才有效果的情况。

+ 2.对于每次输入到网络中的一批样本，其对应的网络结构都是不同的。不同的样本就对应不同的网络。

+ 3.在native bayes中，假设各个特征之间相互独立，这样在训练样本比较少的情况下，单独对每个特征进行学习，测试时将所有的特征相乘，在实际中应用效果不错。



{% include references.md %}
