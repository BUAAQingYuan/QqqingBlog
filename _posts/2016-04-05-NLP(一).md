---
layout: post
title:  NLP(一)
category: nlp
---


## Term Weighting ##

文本分词后，需要对分词后的每一个term计算一个权重。对于文本串的每个term，预测一个[0,1]的得分，得分越大则term重要性越高。Term 

Weighting的打分公式一般由三部分组成:local、global和normalization。

Term Weighting=L_{i,j}G_i N_j

L_{i,j}是term i在document j中的local weight , G_i是term i 的global weight , N_j是document j的归一化因子。

常见的local、global、normalization weight公式有:

![local](http://{{ site.host }}/assets/images/NLP1_local_weight.png "local weight")

![global](http://{{ site.host }}/assets/images/NLP1_global_weight.png "global weight")

![normal](http://{{ site.host }}/assets/images/NLP1_normlization_weight.png "normal weight")

### TF-IDF ###

tf-idf是一种最常见的term weight方法。Tf-Idf的local weight是FREQ，glocal weight是IDFB，normalization是None。tf是词频，表示这个词出现

的次数。df是文档频率，表示这个词在多少个文档中出现。idf则是逆文档频率，idf=log(TD/df)，TD表示总文档数。

## 词向量 ##

![word2vec](http://{{ site.host }}/assets/images/NLP1_word2vec.png "word2vec")

训练语言模型的n-gram算法是利用第m个词的前n个词预测第m个词，而训练词向量是用其前后各n个词来预测第m个词。

word2vec的两种训练方法:

CBOW，训练目标是给定一个word的context，预测word的概率。

skip-gram，训练目标是给定一个word，预测word的context的概率。


### Hierarchical Softmax ###

基于神经网络的语言模型的目标函数通常取为如下对数似然函数

$$ \Gamma =\sum_{w \in C } \log p(w \| Content(w))$$

word2vec中基于Hierarchical Softmax的CBOW模型，优化的目标函数如上；而基于Hierarchical Softmax的skip-gram模型，优化的目标函数为

$$ \Gamma =\sum_{w \in C } \log p(Content(w) \| w)$$

#### CBOW ####



### Negative Sampling ###


{% include references.md %}
