---
layout: post
title:  NLP(一)
category: nlp
---


## Term Weighting ##

文本分词后，需要对分词后的每一个term计算一个权重。对于文本串的每个term，预测一个[0,1]的得分，得分越大则term重要性越高。Term 

Weighting的打分公式一般由三部分组成:local、global和normalization。

Term Weighting=L_{i,j}G_i N_j

L_{i,j}是term i在document j中的local weight , G_i是term i 的global weight , N_j是document j的归一化因子。

常见的local、global、normalization weight公式有:

![local](http://{{ site.host }}/assets/images/NLP1_local_weight.png "local weight")

![global](http://{{ site.host }}/assets/images/NLP1_global_weight.png "global weight")

![normal](http://{{ site.host }}/assets/images/NLP1_normlization_weight.png "normal weight")

### TF-IDF ###

tf-idf是一种最常见的term weight方法。Tf-Idf的local weight是FREQ，glocal weight是IDFB，normalization是None。tf是词频，表示这个词出现

的次数。df是文档频率，表示这个词在多少个文档中出现。idf则是逆文档频率，idf=log(TD/df)，TD表示总文档数。

## 词向量 ##

![word2vec](http://{{ site.host }}/assets/images/NLP1_word2vec.png "word2vec")

训练语言模型的n-gram算法是利用第m个词的前n个词预测第m个词，而训练词向量是用其前后各n个词来预测第m个词。

word2vec的两种训练方法:

CBOW，训练目标是给定一个word的context，预测word的概率。

skip-gram，训练目标是给定一个word，预测word的context的概率。


### Hierarchical Softmax ###

基于神经网络的语言模型的目标函数通常取为如下对数似然函数

$$ \Gamma =\sum_{w \in C } \log p(w \| Content(w))$$

word2vec中基于Hierarchical Softmax的CBOW模型，优化的目标函数如上；而基于Hierarchical Softmax的skip-gram模型，优化的目标函数为

$$ \Gamma =\sum_{w \in C } \log p(Content(w) \| w)$$

#### CBOW ####

CBOW模型的网络结构，包括三层:输入层、投影层和输出层。

![cbow模型](http://{{ site.host }}/assets/images/CBOW.png "CBOW")

假设Context(w）由w前后各c个词构成。

+ 输入层

包含Context(w)中2c个词的词向量$$v(Context(w)_{1}),v(Context(w)_{2}),...,v(Context(w)_{2c}) \in R^{m}$$,m为词向量的维度。

+ 投影层

将输入层的2c个向量做累加求和，$$ X_{w}=\sum_{i=1}^{2c} v(Context(w)_{i}) \in R^{m} $$

+ 输出层

输出层对应一颗二叉树，它以语料中出现过的词当叶子节点，以各词在语料中出现的次数当权值构造出来的Huffman树。在这颗Huffman树中，叶子节点共N=D个，分别对应词典D中词，非叶子节点N-1个。


神经网络语言模型中，模型的大部分计算集中在隐藏层和输出层之间的矩阵向量运算，以及输出层上的softmax归一化运算。

利用Huffman树来定义函数$$ p(w \| Content(w))$$。对于每个词，从根节点出发到达该词的叶子节点，中间经历的每次分支都视为一次二分类。word2vec中将编码为1的节点定义为负类，将编码为0

的类定义为正类。简而言之，将一个节点进行分类时，分到左边是负类，分到右边是正类。逻辑回归中，一个节点被分为正类的概率是

$$ \sigma(X_{w}^{T} \theta) = \frac{1}{1+e^{-x_{w}^{T} \theta}} $$

被分为负类的概率为

$$ 1-\sigma(X_{w}^{T} \theta) $$

第 i 次分类的概率为  $$ p(d_{i}^{w} \|X_{w},\theta_{i-1}^{w})=1-\sigma(X_{w}^{T} \theta_{i}^{w}) 或 \sigma(X_{w}^{T} \theta_{i-1}^{w})$$

则 p(w \| Context(w))=$$ \prod_{i=2}^{l_{w}} p(d_{i}^{w} \|X_{w},\theta_{i-1}^{w}) $$

对于词典D中的任意词w,Huffman树中必存在一条从根节点到词w对应的路径且路径唯一，路径存在的所有分支，每个分支都看做是一次二分类，每一次分类产生一个概率，将这些概率乘起来就是所需

的p(w \| Context(w)) ，带入对数似然函数就得到CBOW模型的目标函数。word2vec采用的是随机梯度上升法优化目标函数。



### Negative Sampling ###


{% include references.md %}
