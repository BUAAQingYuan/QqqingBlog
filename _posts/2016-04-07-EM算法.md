---
layout: post
title: EM算法
category: ml
---

EM算法，期望最大化算法。

对于一个模型而言，令X表示已经观测到的变量集，Z表示隐变量集，$$\theta$$表示模型参数。使用极大似然估计法估计模型参数，

$$\max LL(X,Z,\Theta )=\max \ln P(\Theta | X,Z)\propto \max \ln P(X,Z|\Theta )$$

由于Z是隐变量，无法直接求解。

EM算法是常用的估计参数隐变量的方法，如参数$$\theta$$已知，可根据训练数据X推断出最优隐变量Z的值(E步)；

对Z计算期望，最大化以观测数据X的"边际似然"，

$$\max LL(\theta | X) \propto \max \ln P(X | \theta)=\ln \sum _{Z} P(X,Z | \theta)$$

若Z已知，则可以对参数$$\theta$$做最大似然估计(M步)。

以初始值$$\theta^{0}$$为起点，

1. 基于$$\theta^{t}$$推断隐变量Z的期望，记为$$Z^{t}$$
2. 基于已观测变量X和$$Z^{t}$$对参数$$\theta$$做极大似然估计,记为$$\theta^{t+1}$$

### 常用的EM算法 ###

进一步不取Z的期望，而是基于 $$\theta^{t}$$ 计算隐变量Z的概率分布 P(Z | X,$$\theta^{t}$$ ),EM算法的两个步骤如下。

E步：以当前参数$$\theta^{t}$$推断隐变量分布P(Z | X,$$\theta^{t}$$ )，并计算对数似然 LL($$\theta $$ | X,Z) 的期望

$$ Q(\theta | \theta^{t}) =E_{Z|X,\theta^{t}} LL(\theta | X,Z) $$

M步: 寻找参数最大化期望似然，
		
$$ \theta^{t+1}={\arg \max}_{\theta} Q(\theta | \theta^{t}) $$

EM算法的E步是利用当前估计的参数值来计算对数似然的期望值；M步是寻找能使E步产生的似然期望最大的参数值，然后得到的参数值重新用于E步，直到收敛到局部最优解。

隐变量的估计问题也可通过梯度下降等优化算法求解，但是这种算法的复杂度以隐变量的数目按指数级上升。EM算法则是一种非梯度优化问题。

{% include references.md %}
